## 概述
这篇论文主要是介绍了基于RoCE v2实现的RDMA协议中基于PFC来做拥塞控制，同时介绍了PFC带来的一些问题以及解决方案。

## 精要
传统TCP/IP由于CPU负载高、延迟高，不能满足新一代DC工作负载，作者提出了基于PFC的DSCP，利用RoCEv2（融合以太网的RDMA）来代替TCP进行内部数据中心通信。我们验证了大规模运行的安全性和可扩展性。基于DCSP的优先级流控机制PFC来确保大规模部署，同时解决了很多安全挑战，比如活锁、死锁、PFC暂停风暴、慢接收者等问题，同时也建立了完善的监控和管理系统来确保RDM按照预期来工作。

- RoCE v2的实现：
  - RoCEv2在Ethernet/IPv4/UDP包中封装RDMA协议，使得RoCEv2和我们现成的网络基础设施相兼容，接收者UDP端口固定为4791。
  - PFC被用来防止缓冲区溢出，PFC标准规定了8个优先级种类，但是作者只使用了两个优先级，原因如下(我的理解：缓冲区加大了，PFC触发的阈值提高了)：
    - 8个优先级需要8个队列来分配缓冲区，我们利用两个队列来共享缓冲区。更大的buffer会提高XOFF阈值，保证更平顺的收发速率。
    - DC内部通信距离较短，但是ToR和leaf交换机buffer较浅（10MB左右），所以只够给两个无损的流量队列预留充足的headroom。
- 为什么需要流量控制？
  PFC会带来unfairness和victim flow问题，影响面太大
- 如何解决？
  引入基于流量的控制，作者选择DCQCN，本质上是利用ECN进行拥塞预告，它能够对中间交换机的队列长度进行响应，并且所有交换机都支持ECN。(DSQCN能够尽可能维持队列长度在较低的水平，但是依然阻止不了PFC的产生。)
- DSCP如何实现
  略
- PFC带来的一些挑战
  - livelock：原因是丢包导致不断触发go-back-0，解决办法是使用go-back-N代替go-back-0
  - deadlock：两个子网下都出现了死机，而路由器上洪泛导致对端引起双向PFC暂停双向发送。解决办法是如果数据包对应的ARP条目不完整，我们将其丢弃。
  - NIC PFC pause frame storm：NIC由于接收队列满不停发送PFC给上游节点，导致整个路径上的所有端口都暂停发送。解决办法是交换机和NIC都开发开门狗:
    - NIC微控制器检测到接收管道已停止一段时间（默认为100ms），并且NIC正在生成暂停帧，则微控制器将禁止NIC生成暂停帧。
    - 交换机看门狗监视面向服务器的端口。 一旦面向出口端口的服务器对无法排空的数据包进行排队，并且该端口正在从NIC接收连续的暂停帧，则交换机将禁用该端口的无损模式，并丢弃往返于该端口的无损数据包。
  - slow-receiver症状：NIC内存资源有限，MTT高速缓存未命中将减慢数据包处理流程。 一旦接收管道速度变慢，并且接收缓冲区占用超过PFC阈值，NIC必须为交换机生成PFC暂停帧。解决办法： 在NIC侧使用2MB的大页面(而不是4KB)以降低MTT不命中概率。 在交换机侧启用了不同交换机端口之间的动态缓冲区共享；与静态缓冲区预留相比，动态缓冲区共享在统计上为RDMA流量提供了更多缓冲区。 
- 生产环境使用RDMA
  - 启用管理和监控功能：
    - 交换机侧对每个端口单独配置PFC。
    - 在服务器端，有用于启用/禁用RoCEv2的配置，PFC配置，DCQCN配置和流量配置。 
    - 配置监视服务检查交换机和服务器的运行配置是否与所需配置相同。
  - PFC暂停帧和流量监视：
    - 监视交换机和服务器间发送和接收的暂停帧，在服务器侧监视暂停帧的间隔。
    - 用流量计数器监视入口和出口的流量，另外丢弃计数器帮我们监视是否有丢弃。
  - Pingmesh:  RDMA Pingmesh将有效负载大小为512字节的RDMA探针下发到不同位置（ToR，Podset，数据中心）的服务器，并记录测量的RTT（如果探针成功）或错误代码（如果探针失败）， 利用该测量值可以推断RDMA是否运行良好。
